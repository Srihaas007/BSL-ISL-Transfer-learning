import tensorflow as tf
from tensorflow.keras.applications import VGG16
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# 1. Data Acquisition and Preprocessing
def preprocess_data():
    datagen = ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2,
                                 height_shift_range=0.2, shear_range=0.2, zoom_range=0.2,
                                 horizontal_flip=True, fill_mode='nearest')
    train_generator = datagen.flow_from_directory('path_to_train_data',
                                                  target_size=(224, 224),
                                                  batch_size=32,
                                                  class_mode='categorical')
    validation_generator = datagen.flow_from_directory('path_to_validation_data',
                                                       target_size=(224, 224),
                                                       batch_size=32,
                                                       class_mode='categorical')
    return train_generator, validation_generator

# 2. Loading the Pre-trained VGG16 Model
def load_model():
    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
    x = Flatten()(base_model.output)
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.5)(x)
    predictions = Dense(10, activation='softmax')(x)  # Assuming 10 classes
    model = Model(inputs=base_model.input, outputs=predictions)
    return model

# 3. Base Model Testing
def base_model_testing(model, validation_generator):
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    results = model.evaluate(validation_generator)
    print(f"Test Loss, Test Accuracy: {results}")

# 4. Model Fine-Tuning
def fine_tune_model(model):
    for layer in model.layers[:15]:  # Freeze all layers except the last 4
        layer.trainable = False
    model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# 5. Hyperparameter Tuning
# This is typically more complex and often involves using tools like Keras Tuner or conducting multiple experiments

# 6. Training the Model
def train_model(model, train_generator, validation_generator):
    history = model.fit(train_generator, epochs=10, validation_data=validation_generator)
    return history

# 7. Testing the Model
def test_model(model, test_generator):
    test_loss, test_accuracy = model.evaluate(test_generator)
    return test_loss, test_accuracy

# 8. Evaluation
def evaluate_model(model, test_generator):
    predictions = model.predict(test_generator)
    predicted_classes = np.argmax(predictions, axis=1)
    true_classes = test_generator.classes
    class_labels = list(test_generator.class_indices.keys())   
    report = classification_report(true_classes, predicted_classes, target_names=class_labels)
    print(report)

# Main Execution Flow
train_gen, val_gen = preprocess_data()
model = load_model()
base_model_testing(model, val_gen)
model = fine_tune_model(model)
history = train_model(model, train_gen, val_gen)
test_loss, test_accuracy = test_model(model, val_gen)
evaluate_model(model, val_gen)
